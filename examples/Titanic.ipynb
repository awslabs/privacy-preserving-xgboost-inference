{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Run <code>jupyter notebook</code> under the project directory )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppxgboost import BoosterParser as boostparser\n",
    "from ppxgboost import PPBooster as ppbooster\n",
    "from ppxgboost import PaillierAPI as paillier\n",
    "from ppxgboost.PPBooster import MetaData\n",
    "import ppxgboost.PPKey as PPKey\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from secrets import token_bytes\n",
    "import pyope.ope as pyope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# XGBoost for Titanic Dataset\n",
    "\n",
    "(We use this example to demenstrate how to use ppxgboost for encypting an xgboost model and query it.)\n",
    "\n",
    "Please go to https://www.kaggle.com/c/titanic/data and download the dataset.\n",
    "In the following example, the datasets are downloaded in the example directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Preparation and Train an XGBoost ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pp-xgboost for titanic \n",
    "# In the following example, the datasets are downloaded in the example directory\n",
    "train = pd.read_csv('../data/titanic_train.csv')\n",
    "test = pd.read_csv('../data/titanic_test.csv')\n",
    "\n",
    "# Training dataset. We skip the data exploration part ...\n",
    "# Only get the features that are useful for building the ML model\n",
    "X_train = train[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']]\n",
    "y_train = train[['Survived']]\n",
    "\n",
    "# Testing dataset\n",
    "X_test = test[['Pclass', 'Age', 'Fare', 'SibSp', 'Parch']]\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a xgboost model \n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "params = {'eta': 0.1}\n",
    "model = xgb.train(params=params, dtrain=dtrain)\n",
    "\n",
    "# predict using the plaintext prediction\n",
    "plaintext_predict = model.predict(xgb.DMatrix(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump_model('tree.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encryption Preparation for XGBoost Model\n",
    "1. Set up some metadata information for the dataset.\n",
    "2. Set up the encryption materials\n",
    "3. Encrypt the model\n",
    "4. Encrypt the query\n",
    "5. Perform the prediction\n",
    "6. Decrypt the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Create custom data ranges\n",
    "in_range = pyope.ValueRange(pyope.DEFAULT_IN_RANGE_START, 2 ** 43 - 1)\n",
    "out_range = pyope.ValueRange(pyope.DEFAULT_OUT_RANGE_START, 2 ** 63 - 1)\n",
    "\n",
    "# 1. parsing to internal tree data structure, and output feature set\n",
    "min_max = boostparser.training_dataset_parser(X_test)\n",
    "enc_tree, feature_set, min_max = boostparser.model_to_trees(model, min_max)\n",
    "\n",
    "# 2. Set up encryption materials.\n",
    "ppModelKey, ppQueryKey = PPKey.generatePPXGBoostKeys()\n",
    "\n",
    "# 3. process the tree into enc_tree\n",
    "ppbooster.enc_xgboost_model(ppModelKey, enc_tree, MetaData(min_max, in_range.end))\n",
    "\n",
    "\n",
    "# 4. Encrypts the input vector for prediction (using prf_key_hash and ope-encrypter) based on the feature set.\n",
    "ppbooster.enc_input_vector(ppQueryKey.get_prf_key(), ppQueryKey.get_ope_encryptor(), feature_set, X_test, MetaData(min_max, in_range.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. privacy-preserving evaluation.\n",
    "start = time.time()\n",
    "values = ppbooster.predict_binary(enc_tree, X_test)\n",
    "end = time.time()\n",
    "print(\"Elapsed Time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. decryption\n",
    "decryptions = []\n",
    "\n",
    "for c in values:\n",
    "    decryptions.append(paillier.decrypt(ppQueryKey.get_private_key(), c))\n",
    "\n",
    "decryptions = np.array([round(x, 7) for x in decryptions])\n",
    "assert len(plaintext_predict) == len(decryptions)\n",
    "\n",
    "# if the predicted values are same (the ppxgboost might not produce same values \n",
    "#                                    as the plaintext value due to precision)\n",
    "for i in range(len(plaintext_predict)):\n",
    "    assert abs(plaintext_predict[i] - decryptions[i]) < 0.000001"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
